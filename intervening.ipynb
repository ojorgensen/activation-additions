{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.extract_utils import average_vectors, gather_activations_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, accelerate, einops, json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.model_utils import load_gpt_model_and_tokeniser\n",
    "\n",
    "from src.utils.extract_utils import create_steering_vector\n",
    "\n",
    "from src.utils.intervention_utils import steering_natural_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.68s/it]\n",
      "/root/activation-additions-large-models/myenv/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, MODEL_CONFIG = load_gpt_model_and_tokeniser(model_name=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.intervention_utils as iu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = {}\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('datasets/fantasy.json', 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_fantasy = json.load(file)\n",
    "\n",
    "  stories[\"fantasy\"] = dataset_fantasy\n",
    "\n",
    "with open('datasets/scifi.json', 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_scifi = json.load(file)\n",
    "\n",
    "  stories[\"scifi\"] = dataset_scifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset_utils import read_all_text_files\n",
    "\n",
    "training_dataset = read_all_text_files(\"datasets/opentext_subset\")\n",
    "\n",
    "# Cut texts for first 200 tokens\n",
    "# Determine the cutoff point using the tokenizer\n",
    "training_dataset = [tokenizer.decode(tokenizer.encode(text)[:200])[4:] for text in training_dataset][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Steering Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering activations: 100%|██████████| 200/200 [00:46<00:00,  4.26it/s]\n",
      "Gathering activations: 100%|██████████| 400/400 [01:47<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "steering_vector = create_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    MODEL_CONFIG,\n",
    "    dataset_fantasy,\n",
    "    training_dataset,\n",
    "    [\"layer_hook_names\"],\n",
    "    False,\n",
    "    False,\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Steering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Clearlu should just make num_beams smaller!!!\n",
    "\n",
    "outputs = steering_natural_text(\n",
    "    \"Here is a story:\", \n",
    "    28,\n",
    "    steering_vector[28] * 100, \n",
    "    model,\n",
    "    MODEL_CONFIG, \n",
    "    tokenizer, \n",
    "    max_new_tokens=100, \n",
    "    temperature=1.0, \n",
    "    freq_penalty=2.0,\n",
    "    top_p=0.3,\n",
    "    n_completions=2,\n",
    "    n_beams=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'main probably going public top starting really major either heading ( registered heads American kill chief early actually attack at immediate leading ge first general hit because right minimum GET probable very fran get serious national substant worth nicely basically front relative commercial policy incoming two following next big area work flat got after related ($ register deg on priv zeit hits fresh majority — move drama bo coming “ apo same have working(! Mass back excellent up bottom mediarès behaviour support sue (! exec vice┌ hitting finishedkill semantics Public pretty based健 check goes here'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"steered\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[\"steered\"][0].squeeze()[-max:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
